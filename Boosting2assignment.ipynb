{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1f727b-b47c-4996-959f-0054e5110bea",
   "metadata": {},
   "source": [
    "Q1. **What is Gradient Boosting Regression?**\n",
    "   Gradient Boosting Regression is a machine learning technique that builds an ensemble of decision trees sequentially. Unlike traditional decision tree algorithms like Random Forest, where trees are built independently, in gradient boosting, each tree is built to correct errors made by the previous trees. It optimizes a loss function (often the mean squared error for regression tasks) by iteratively adding new models to the ensemble.\n",
    "\n",
    "Q2. **Implementing a Simple Gradient Boosting Algorithm from Scratch:**\n",
    "   Below is a simple implementation of Gradient Boosting Regression using Python and NumPy:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        residuals = y.copy()\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "            predictions = tree.predict(X)\n",
    "            residuals -= self.learning_rate * predictions\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(len(X))\n",
    "        for tree in self.trees:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# Example usage\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X.squeeze() + np.random.randn(100)\n",
    "\n",
    "# Fit gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X, y)\n",
    "\n",
    "# Evaluate model\n",
    "predictions = gb_regressor.predict(X)\n",
    "mse = mean_squared_error(y, predictions)\n",
    "r2 = r2_score(y, predictions)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "```\n",
    "\n",
    "Q3. **Experimenting with Different Hyperparameters:**\n",
    "   You can experiment with different hyperparameters like learning rate, number of trees, and tree depth by adjusting them in the initialization of the `GradientBoostingRegressor` class. To find the best hyperparameters, you can use techniques like grid search or random search.\n",
    "\n",
    "Q4. **What is a Weak Learner in Gradient Boosting?**\n",
    "   In Gradient Boosting, a weak learner is a model that performs slightly better than random guessing on a given problem. In the context of Gradient Boosting Regression, decision trees with shallow depths (often referred to as decision stumps) are commonly used as weak learners.\n",
    "\n",
    "Q5. **Intuition behind the Gradient Boosting Algorithm:**\n",
    "   The intuition behind Gradient Boosting is to iteratively improve the model by adding weak learners to correct the errors made by the existing ensemble. Each new model focuses on the mistakes made by the previous models, gradually reducing the overall error.\n",
    "\n",
    "Q6. **Building an Ensemble of Weak Learners in Gradient Boosting:**\n",
    "   Gradient Boosting builds an ensemble of weak learners sequentially. Each new weak learner is trained on the residuals (the differences between the actual and predicted values) of the previous ensemble. The predictions of all weak learners are combined using a weighted sum to make the final prediction.\n",
    "\n",
    "Q7. **Steps Involved in Constructing the Mathematical Intuition of Gradient Boosting Algorithm:**\n",
    "   1. **Initialization**: Start with an initial prediction (often the mean of the target variable).\n",
    "   2. **Compute Residuals**: Calculate the difference between the actual values and the current prediction.\n",
    "   3. **Fit a Weak Learner to Residuals**: Train a weak learner (e.g., decision tree) to predict the residuals.\n",
    "   4. **Update the Prediction**: Update the current prediction by adding a scaled version of the predictions from the weak learner.\n",
    "   5. **Repeat**: Iterate the process by computing new residuals, fitting new weak learners to these residuals, and updating the predictions until a stopping criterion is met or until the desired number of weak learners is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755fc3a4-489e-4242-a863-ea26f54e1465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
