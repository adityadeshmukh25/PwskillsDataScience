{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad02c87f-2e50-40fe-b1ad-445e9e05f93e",
   "metadata": {},
   "source": [
    "Q1. The K-Nearest Neighbors (KNN) algorithm is a simple yet effective supervised learning algorithm used for classification and regression tasks. It works based on the assumption that similar data points tend to fall into similar categories or have similar values.\n",
    "\n",
    "Q2. The value of K in KNN is chosen empirically based on cross-validation techniques or other validation methods. Typically, you try different values of K and choose the one that provides the best performance on unseen data. Choosing K too small may lead to overfitting, while choosing it too large may lead to underfitting.\n",
    "\n",
    "Q3. The main difference between KNN classifier and KNN regressor lies in their tasks:\n",
    "   - KNN classifier predicts the class label of a data point based on the majority class among its K nearest neighbors.\n",
    "   - KNN regressor predicts the value of a data point based on the average or weighted average of the values of its K nearest neighbors.\n",
    "\n",
    "Q4. The performance of KNN can be measured using various evaluation metrics such as accuracy, precision, recall, F1-score for classification tasks, and metrics like Mean Squared Error (MSE) or Mean Absolute Error (MAE) for regression tasks.\n",
    "\n",
    "Q5. The curse of dimensionality in KNN refers to the phenomenon where the feature space becomes increasingly sparse as the number of dimensions (features) increases. This can lead to poor performance of KNN as the notion of proximity becomes less meaningful in high-dimensional spaces.\n",
    "\n",
    "Q6. Handling missing values in KNN involves either imputing them using techniques like mean, median, or mode imputation, or using more advanced methods like KNN-based imputation where the missing values are estimated based on the values of the nearest neighbors.\n",
    "\n",
    "Q7. The performance of KNN classifier and regressor depends on the nature of the problem:\n",
    "   - KNN classifier is better suited for classification tasks where the decision boundaries are nonlinear and the data is not highly dimensional.\n",
    "   - KNN regressor is better suited for regression tasks where the relationship between the features and the target variable is continuous and can be approximated well by local interpolation.\n",
    "\n",
    "Q8. \n",
    "   - Strengths:\n",
    "     - Simple to understand and implement.\n",
    "     - No training phase; the model memorizes the training data.\n",
    "     - Effective for non-linear data and non-parametric problems.\n",
    "   - Weaknesses:\n",
    "     - Computationally expensive during prediction, especially with large datasets or high-dimensional feature spaces.\n",
    "     - Sensitive to irrelevant or redundant features.\n",
    "     - Performance can degrade in the presence of noisy data or imbalanced classes.\n",
    "   These weaknesses can be addressed by employing dimensionality reduction techniques, feature selection, using distance weighting, or utilizing faster search algorithms like KD-trees.\n",
    "\n",
    "Q9. The difference between Euclidean distance and Manhattan distance lies in their calculation:\n",
    "   - Euclidean distance is the straight-line distance between two points in Euclidean space, calculated as the square root of the sum of the squared differences between corresponding coordinates.\n",
    "   - Manhattan distance (also known as city block distance or L1 distance) is the sum of the absolute differences between corresponding coordinates of the points.\n",
    "   In KNN, Euclidean distance is more commonly used, but Manhattan distance can be preferred in certain scenarios, especially when dealing with high-dimensional data or when the features are not on the same scale.\n",
    "\n",
    "Q10. Feature scaling in KNN is essential because KNN relies on distance metrics to find nearest neighbors. Feature scaling ensures that all features contribute equally to the distance computation. Common scaling techniques include Min-Max scaling (scaling features to a range of [0,1]) or standardization (scaling features to have a mean of 0 and a standard deviation of 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36da414c-a5e4-4f1d-a446-8a1e486e71bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
