{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "186be100-9aa4-4e82-a13b-8829e035aeaa",
   "metadata": {},
   "source": [
    "### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "\n",
    "**Simple Linear Regression:** In simple linear regression, there is a single independent variable used to predict a dependent variable. For instance, predicting house prices (dependent variable) based on just one predictor, such as square footage (independent variable).\n",
    "\n",
    "**Multiple Linear Regression:** Multiple linear regression involves multiple independent variables to predict a dependent variable. For example, predicting house prices based on square footage, number of bedrooms, and distance from the city center.\n",
    "\n",
    "### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "\n",
    "Assumptions of linear regression include linearity, independence of errors, homoscedasticity (constant variance of errors), and normality of errors. You can check these assumptions using diagnostic plots (e.g., residual plots), statistical tests (e.g., Shapiro-Wilk test for normality), and examining patterns in the data.\n",
    "\n",
    "### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "\n",
    "- **Slope:** It represents the change in the dependent variable for a one-unit change in the independent variable while holding other variables constant.\n",
    "- **Intercept:** It indicates the value of the dependent variable when all independent variables are zero.\n",
    "\n",
    "For instance, in a house price prediction model, the slope of the square footage variable represents the change in price for each additional square foot, while the intercept could represent the base price of a house with zero square footage.\n",
    "\n",
    "### Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function in machine learning models. It iteratively updates the model parameters (weights) by moving in the direction of the steepest descent of the cost function, aiming to reach the minimum point (global or local). This process involves calculating the gradient (derivative) of the cost function with respect to the model parameters.\n",
    "\n",
    "### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "Multiple linear regression involves predicting a dependent variable using multiple independent variables. It differs from simple linear regression, which has only one independent variable. The equation for multiple linear regression includes multiple coefficients for each independent variable.\n",
    "\n",
    "### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n",
    "Multicollinearity occurs when independent variables in a regression model are highly correlated. It can cause issues in interpreting coefficients. Detecting multicollinearity can be done using correlation matrices or variance inflation factor (VIF). To address it, you can consider removing variables causing high multicollinearity, using dimensionality reduction techniques, or combining correlated variables.\n",
    "\n",
    "### Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "Polynomial regression is a type of regression analysis where the relationship between the independent variable(s) and dependent variable is modeled as an nth degree polynomial. It differs from linear regression as it allows for curved relationships between variables by introducing polynomial terms (e.g., quadratic, cubic) into the model equation.\n",
    "\n",
    "### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "**Advantages:**\n",
    "- Captures non-linear relationships better than linear regression.\n",
    "- Flexibility in modeling complex relationships.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Tendency to overfit the data.\n",
    "- Interpretability becomes challenging with higher-degree polynomials.\n",
    "\n",
    "Use polynomial regression when there's evidence of a non-linear relationship between variables. However, be cautious as higher-degree polynomials can introduce complexities and overfitting, particularly with smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac54cd-8a4d-4624-b203-a7236f89b4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
