{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f60fde-b829-4558-9501-e800f7fd350e",
   "metadata": {},
   "source": [
    "Q1. An ensemble technique in machine learning involves combining multiple models to improve predictive performance. These models can be of the same or different types and are trained on the same dataset.\n",
    "\n",
    "Q2. Ensemble techniques are used in machine learning for several reasons:\n",
    "   - They often lead to better predictive performance compared to individual models.\n",
    "   - They can reduce overfitting by combining multiple models.\n",
    "   - They can handle complex relationships in the data more effectively.\n",
    "   - They provide more robustness and stability to the model's predictions.\n",
    "\n",
    "Q3. Bagging, or Bootstrap Aggregating, is an ensemble technique where multiple models are trained on different random subsets of the training data with replacement. The final prediction is typically obtained by averaging the predictions of all models (for regression) or using voting (for classification).\n",
    "\n",
    "Q4. Boosting is an ensemble technique where multiple weak learners are combined to create a strong learner. It works by sequentially training models, with each subsequent model giving more weight to the instances that the previous models misclassified. Examples of boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost.\n",
    "\n",
    "Q5. The benefits of using ensemble techniques include:\n",
    "   - Improved predictive performance.\n",
    "   - Reduction of overfitting.\n",
    "   - Handling complex relationships in data.\n",
    "   - Providing robustness and stability to predictions.\n",
    "\n",
    "Q6. Ensemble techniques are not always better than individual models. Their effectiveness depends on various factors such as the diversity of base models, the quality of data, and the problem domain. In some cases, a well-tuned individual model may perform better than an ensemble.\n",
    "\n",
    "Q7. The confidence interval using bootstrap is calculated by resampling the original dataset with replacement multiple times, computing the statistic of interest (e.g., mean, median) for each resampled dataset, and then calculating the desired percentile of the distribution of these statistics.\n",
    "\n",
    "Q8. Bootstrap is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly resampling the dataset with replacement. The steps involved in bootstrap are:\n",
    "   1. Randomly draw a sample of the same size as the original dataset from the dataset with replacement.\n",
    "   2. Calculate the statistic of interest (e.g., mean, median) for the resampled dataset.\n",
    "   3. Repeat steps 1 and 2 a large number of times (e.g., 1000 or more).\n",
    "   4. Compute the desired percentile of the distribution of the statistics to obtain the confidence interval.\n",
    "\n",
    "Q9. To estimate the 95% confidence interval for the population mean height using bootstrap:\n",
    "   - Randomly sample 50 heights with replacement from the observed sample.\n",
    "   - Calculate the mean height for each bootstrap sample.\n",
    "   - Repeat the above steps a large number of times (e.g., 1000).\n",
    "   - Compute the 2.5th and 97.5th percentiles of the bootstrap means to obtain the 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a744853d-8d27-476f-9bca-93ed3d3bec74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
