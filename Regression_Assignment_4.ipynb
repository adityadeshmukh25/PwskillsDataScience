{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659ab5df-d548-403f-8b20-51809b31d00c",
   "metadata": {},
   "source": [
    "### Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "**Lasso Regression (Least Absolute Shrinkage and Selection Operator)** is a type of linear regression that adds a penalty term (L1 regularization) to the standard linear regression cost function. This penalty term is proportional to the absolute value of the coefficients, forcing some coefficients to be exactly zero. This property of setting coefficients to zero makes Lasso useful for feature selection by effectively performing variable selection and regularization.\n",
    "\n",
    "Differences from other regression techniques:\n",
    "- **Ridge Regression vs. Lasso:** Ridge Regression uses L2 regularization, which penalizes the sum of the squares of coefficients. Unlike Lasso, Ridge Regression typically does not lead to exact zero coefficients.\n",
    "- **Elastic Net Regression vs. Lasso:** Elastic Net combines L1 and L2 regularization, providing a balance between Lasso and Ridge by including penalties from both.\n",
    "\n",
    "### Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "The main advantage of Lasso Regression in feature selection is its ability to automatically perform feature selection by driving some coefficients to exactly zero. This property allows for the elimination of irrelevant or less important features from the model, leading to a simpler and more interpretable model with potentially better generalization to new data.\n",
    "\n",
    "### Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "Interpreting Lasso coefficients is similar to interpreting coefficients in linear regression. The coefficient magnitude signifies the strength of the relationship between the predictor variable and the target variable. However, due to L1 regularization, coefficients can be exactly zero, indicating that the corresponding feature has been excluded from the model.\n",
    "\n",
    "### Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "\n",
    "The main tuning parameter in Lasso Regression is the regularization parameter (often denoted as lambda or alpha). Higher values of lambda increase the penalty for larger coefficient values, potentially shrinking more coefficients to zero and encouraging sparsity in the model. A lower value of lambda decreases the penalty, allowing more features to be included in the model.\n",
    "\n",
    "### Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "Lasso Regression, in its standard form, is a linear regression technique. However, it can be extended to handle non-linear regression problems by employing techniques like polynomial feature expansion. By creating polynomial features (e.g., square, cube of existing features), Lasso Regression can capture non-linear relationships between predictors and the target variable.\n",
    "\n",
    "### Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "The primary difference lies in the type of regularization:\n",
    "- **Ridge Regression** uses L2 regularization, penalizing the sum of squares of coefficients.\n",
    "- **Lasso Regression** uses L1 regularization, penalizing the sum of absolute values of coefficients.\n",
    "- Ridge tends to shrink coefficients towards zero but rarely sets them exactly to zero, whereas Lasso can lead to exact zero coefficients, effectively performing variable selection.\n",
    "\n",
    "### Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "Lasso Regression has a feature selection property that tends to automatically select one feature among highly correlated features and set the coefficients of the others to zero. So, in practice, Lasso can help in handling multicollinearity by effectively choosing one among correlated features while reducing others to zero.\n",
    "\n",
    "### Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\n",
    "The optimal value of the regularization parameter in Lasso Regression can be chosen through techniques like cross-validation. Using techniques such as k-fold cross-validation, you can evaluate the model's performance for different values of lambda and select the one that provides the best balance between model simplicity (fewer features) and performance (good predictive ability). Grid search or algorithms like coordinate descent can also be used to find the optimal lambda value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb308b4-c4f6-48a6-8705-782880dc022a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
